{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOLWsgmTIh2KC4z2AE9E1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvathysarat/gpt2-text-generation/blob/master/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhWmzd1llZsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "9fc54de9-8caf-4251-93f6-faef94f3f0ba"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "os.chdir('transformers')\n",
        "print(os.getcwd())\n",
        "!pip install ."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 28.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c97418360ffe92bf911645ceec1bd812d26b7d58221b42de3e7e9195d35edd1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 38349 (delta 9), reused 9 (delta 2), pack-reused 38327\u001b[K\n",
            "Receiving objects: 100% (38349/38349), 27.74 MiB | 26.13 MiB/s, done.\n",
            "Resolving deltas: 100% (26569/26569), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjlxwycXspwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('examples/language-modeling/')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CnhcYntssjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdd9c393-eaa1-4bb9-e502-04d79f099e56"
      },
      "source": [
        "N=1\n",
        "try:os.mkdir('../../../model')\n",
        "except:pass\n",
        "OUTPUT_DIR='../../../model/'\n",
        "print(os.getcwd())\n",
        "TRAIN_FILE='../../../dataset/train.txt'\n",
        "VALID_FILE='../../../dataset/valid.txt'\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers/examples/language-modeling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaTfjcvNtfEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6239edf-1632-44a1-fc57-84dd79ac3d6e"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
        "!python run_language_modeling.py \\\n",
        "--output_dir=$OUTPUT_DIR \\\n",
        "--model_type=gpt2 \\\n",
        "--model_name_or_path=gpt2 \\\n",
        "--do_train \\\n",
        "--train_data_file=$TRAIN_FILE \\\n",
        "--do_eval \\\n",
        "--eval_data_file=$VALID_FILE \\\n",
        "--per_device_train_batch_size=2 \\\n",
        "--per_device_eval_batch_size=2 \\\n",
        "--line_by_line \\\n",
        "--evaluate_during_training \\\n",
        "--learning_rate=5e-5 \\\n",
        "--num_train_epochs=5 \\\n",
        "--overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-16 05:44:22.966271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "08/16/2020 05:44:24 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "08/16/2020 05:44:24 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "08/16/2020 05:44:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../model/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Aug16_05-44-24_39510ae949d6', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None)\n",
            "08/16/2020 05:44:24 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "08/16/2020 05:44:24 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/16/2020 05:44:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "08/16/2020 05:44:25 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/16/2020 05:44:25 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "08/16/2020 05:44:25 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:819: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "08/16/2020 05:44:25 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "08/16/2020 05:44:30 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "08/16/2020 05:44:30 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils_base -   Assigning <BOS> to the bos_token key of the tokenizer\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils -   Adding <BOS> to the vocabulary\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils_base -   Assigning <EOS> to the eos_token key of the tokenizer\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils -   Adding <EOS> to the vocabulary\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils_base -   Assigning <PAD> to the pad_token key of the tokenizer\n",
            "08/16/2020 05:44:30 - INFO - transformers.tokenization_utils -   Adding <PAD> to the vocabulary\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1319: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "08/16/2020 05:44:31 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at train.txt\n",
            "08/16/2020 05:44:32 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at valid.txt\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:189: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -   To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -   ***** Running training *****\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Num examples = 2752\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Num Epochs = 5\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Instantaneous batch size per device = 2\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "08/16/2020 05:44:32 - INFO - transformers.trainer -     Total optimization steps = 6880\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1376 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/1376 [00:06<2:17:41,  6.01s/it]\u001b[A\n",
            "Iteration:   0% 2/1376 [00:13<2:26:24,  6.39s/it]\u001b[A\n",
            "Iteration:   0% 3/1376 [00:23<2:50:22,  7.45s/it]\u001b[A\n",
            "Iteration:   0% 4/1376 [00:31<2:55:57,  7.69s/it]\u001b[A\n",
            "Iteration:   0% 5/1376 [00:33<2:18:40,  6.07s/it]\u001b[A\n",
            "Iteration:   0% 6/1376 [00:35<1:50:30,  4.84s/it]\u001b[A\n",
            "Iteration:   1% 7/1376 [00:37<1:28:47,  3.89s/it]\u001b[A\n",
            "Iteration:   1% 8/1376 [00:39<1:13:22,  3.22s/it]\u001b[A\n",
            "Iteration:   1% 9/1376 [00:40<1:03:41,  2.80s/it]\u001b[A\n",
            "Iteration:   1% 10/1376 [00:42<53:45,  2.36s/it] \u001b[A\n",
            "Iteration:   1% 11/1376 [00:44<51:39,  2.27s/it]\u001b[A\n",
            "Iteration:   1% 12/1376 [00:45<47:41,  2.10s/it]\u001b[A\n",
            "Iteration:   1% 13/1376 [00:47<43:48,  1.93s/it]\u001b[A\n",
            "Iteration:   1% 14/1376 [00:49<41:28,  1.83s/it]\u001b[A\n",
            "Iteration:   1% 15/1376 [00:50<39:11,  1.73s/it]\u001b[A\n",
            "Iteration:   1% 16/1376 [00:52<39:41,  1.75s/it]\u001b[A\n",
            "Iteration:   1% 17/1376 [00:54<41:57,  1.85s/it]\u001b[A\n",
            "Iteration:   1% 18/1376 [00:55<38:36,  1.71s/it]\u001b[A\n",
            "Iteration:   1% 19/1376 [00:57<37:56,  1.68s/it]\u001b[A\n",
            "Iteration:   1% 20/1376 [00:59<40:20,  1.78s/it]\u001b[A\n",
            "Iteration:   2% 21/1376 [01:01<41:58,  1.86s/it]\u001b[A\n",
            "Iteration:   2% 22/1376 [01:04<52:32,  2.33s/it]\u001b[A\n",
            "Iteration:   2% 23/1376 [01:06<46:24,  2.06s/it]\u001b[A\n",
            "Iteration:   2% 24/1376 [01:08<45:34,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 25/1376 [01:09<40:38,  1.80s/it]\u001b[A\n",
            "Iteration:   2% 26/1376 [01:11<38:29,  1.71s/it]\u001b[A\n",
            "Iteration:   2% 27/1376 [01:12<37:50,  1.68s/it]\u001b[A\n",
            "Iteration:   2% 28/1376 [01:14<38:15,  1.70s/it]\u001b[A\n",
            "Iteration:   2% 29/1376 [01:16<40:59,  1.83s/it]\u001b[A\n",
            "Iteration:   2% 30/1376 [01:18<38:27,  1.71s/it]\u001b[A\n",
            "Iteration:   2% 31/1376 [01:20<46:03,  2.05s/it]\u001b[A\n",
            "Iteration:   2% 32/1376 [01:22<43:25,  1.94s/it]\u001b[A\n",
            "Iteration:   2% 33/1376 [01:23<39:59,  1.79s/it]\u001b[A\n",
            "Iteration:   2% 34/1376 [01:25<40:53,  1.83s/it]\u001b[A\n",
            "Iteration:   3% 35/1376 [01:27<42:05,  1.88s/it]\u001b[A\n",
            "Iteration:   3% 36/1376 [01:29<39:35,  1.77s/it]\u001b[A\n",
            "Iteration:   3% 37/1376 [01:31<38:23,  1.72s/it]\u001b[A\n",
            "Iteration:   3% 38/1376 [01:32<36:55,  1.66s/it]\u001b[A\n",
            "Iteration:   3% 39/1376 [01:34<39:19,  1.76s/it]\u001b[A\n",
            "Iteration:   3% 40/1376 [01:36<38:31,  1.73s/it]\u001b[A\n",
            "Iteration:   3% 41/1376 [01:37<37:24,  1.68s/it]\u001b[A\n",
            "Iteration:   3% 42/1376 [01:39<38:56,  1.75s/it]\u001b[A\n",
            "Iteration:   3% 43/1376 [01:41<40:28,  1.82s/it]\u001b[A\n",
            "Iteration:   3% 44/1376 [01:44<43:58,  1.98s/it]\u001b[A\n",
            "Iteration:   3% 45/1376 [01:46<44:57,  2.03s/it]\u001b[A\n",
            "Iteration:   3% 46/1376 [01:48<44:23,  2.00s/it]\u001b[A\n",
            "Iteration:   3% 47/1376 [01:50<46:05,  2.08s/it]\u001b[A\n",
            "Iteration:   3% 48/1376 [01:52<49:10,  2.22s/it]\u001b[A\n",
            "Iteration:   4% 49/1376 [01:55<48:29,  2.19s/it]\u001b[A\n",
            "Iteration:   4% 50/1376 [01:56<42:37,  1.93s/it]\u001b[A\n",
            "Iteration:   4% 51/1376 [01:58<41:05,  1.86s/it]\u001b[A\n",
            "Iteration:   4% 52/1376 [01:59<39:35,  1.79s/it]\u001b[A\n",
            "Iteration:   4% 53/1376 [02:01<38:53,  1.76s/it]\u001b[A\n",
            "Iteration:   4% 54/1376 [02:02<36:58,  1.68s/it]\u001b[A\n",
            "Iteration:   4% 55/1376 [02:04<37:02,  1.68s/it]\u001b[A\n",
            "Iteration:   4% 56/1376 [02:06<36:45,  1.67s/it]\u001b[A\n",
            "Iteration:   4% 57/1376 [02:08<40:43,  1.85s/it]\u001b[A\n",
            "Iteration:   4% 58/1376 [02:09<38:05,  1.73s/it]\u001b[A\n",
            "Iteration:   4% 59/1376 [02:11<38:41,  1.76s/it]\u001b[A\n",
            "Iteration:   4% 60/1376 [02:13<38:33,  1.76s/it]\u001b[A\n",
            "Iteration:   4% 61/1376 [02:15<41:25,  1.89s/it]\u001b[A\n",
            "Iteration:   5% 62/1376 [02:17<38:22,  1.75s/it]\u001b[A\n",
            "Iteration:   5% 63/1376 [02:20<45:41,  2.09s/it]\u001b[A\n",
            "Iteration:   5% 64/1376 [02:21<43:03,  1.97s/it]\u001b[A\n",
            "Iteration:   5% 65/1376 [02:23<40:48,  1.87s/it]\u001b[A\n",
            "Iteration:   5% 66/1376 [02:24<37:55,  1.74s/it]\u001b[A\n",
            "Iteration:   5% 67/1376 [02:26<35:37,  1.63s/it]\u001b[A\n",
            "Iteration:   5% 68/1376 [02:28<37:10,  1.71s/it]\u001b[A\n",
            "Iteration:   5% 69/1376 [02:30<39:05,  1.79s/it]\u001b[A\n",
            "Iteration:   5% 70/1376 [02:32<43:30,  2.00s/it]\u001b[A\n",
            "Iteration:   5% 71/1376 [02:34<44:07,  2.03s/it]\u001b[A\n",
            "Iteration:   5% 72/1376 [02:36<45:18,  2.09s/it]\u001b[A\n",
            "Iteration:   5% 73/1376 [02:38<43:30,  2.00s/it]\u001b[A\n",
            "Iteration:   5% 74/1376 [02:40<45:06,  2.08s/it]\u001b[A\n",
            "Iteration:   5% 75/1376 [02:43<45:17,  2.09s/it]\u001b[A\n",
            "Iteration:   6% 76/1376 [02:44<40:34,  1.87s/it]\u001b[A\n",
            "Iteration:   6% 77/1376 [02:45<38:48,  1.79s/it]\u001b[A\n",
            "Iteration:   6% 78/1376 [02:47<38:59,  1.80s/it]\u001b[A\n",
            "Iteration:   6% 79/1376 [02:49<37:47,  1.75s/it]\u001b[A\n",
            "Iteration:   6% 80/1376 [02:50<35:06,  1.63s/it]\u001b[A\n",
            "Iteration:   6% 81/1376 [02:52<34:45,  1.61s/it]\u001b[A\n",
            "Iteration:   6% 82/1376 [02:54<35:35,  1.65s/it]\u001b[A\n",
            "Iteration:   6% 83/1376 [02:55<36:14,  1.68s/it]\u001b[A\n",
            "Iteration:   6% 84/1376 [02:57<37:49,  1.76s/it]\u001b[A\n",
            "Iteration:   6% 85/1376 [03:00<42:07,  1.96s/it]\u001b[A\n",
            "Iteration:   6% 86/1376 [03:01<39:42,  1.85s/it]\u001b[A\n",
            "Iteration:   6% 87/1376 [03:04<44:59,  2.09s/it]\u001b[A\n",
            "Iteration:   6% 88/1376 [03:06<44:31,  2.07s/it]\u001b[A\n",
            "Iteration:   6% 89/1376 [03:08<44:43,  2.09s/it]\u001b[A\n",
            "Iteration:   7% 90/1376 [03:10<40:56,  1.91s/it]\u001b[A\n",
            "Iteration:   7% 91/1376 [03:11<40:38,  1.90s/it]\u001b[A\n",
            "Iteration:   7% 92/1376 [03:13<40:03,  1.87s/it]\u001b[A\n",
            "Iteration:   7% 93/1376 [03:15<40:39,  1.90s/it]\u001b[A\n",
            "Iteration:   7% 94/1376 [03:17<40:09,  1.88s/it]\u001b[A\n",
            "Iteration:   7% 95/1376 [03:19<38:15,  1.79s/it]\u001b[A\n",
            "Iteration:   7% 96/1376 [03:20<35:54,  1.68s/it]\u001b[A\n",
            "Iteration:   7% 97/1376 [03:22<36:09,  1.70s/it]\u001b[A\n",
            "Iteration:   7% 98/1376 [03:23<34:42,  1.63s/it]\u001b[A\n",
            "Iteration:   7% 99/1376 [03:25<37:16,  1.75s/it]\u001b[A\n",
            "Iteration:   7% 100/1376 [03:27<35:57,  1.69s/it]\u001b[A\n",
            "Iteration:   7% 101/1376 [03:28<33:59,  1.60s/it]\u001b[A\n",
            "Iteration:   7% 102/1376 [03:30<35:56,  1.69s/it]\u001b[A\n",
            "Iteration:   7% 103/1376 [03:32<38:27,  1.81s/it]\u001b[A\n",
            "Iteration:   8% 104/1376 [03:34<36:12,  1.71s/it]\u001b[A\n",
            "Iteration:   8% 105/1376 [03:36<37:53,  1.79s/it]\u001b[A\n",
            "Iteration:   8% 106/1376 [03:37<36:44,  1.74s/it]\u001b[A\n",
            "Iteration:   8% 107/1376 [03:39<35:46,  1.69s/it]\u001b[A\n",
            "Iteration:   8% 108/1376 [03:41<39:36,  1.87s/it]\u001b[A\n",
            "Iteration:   8% 109/1376 [03:43<39:28,  1.87s/it]\u001b[A\n",
            "Iteration:   8% 110/1376 [03:45<38:33,  1.83s/it]\u001b[A\n",
            "Iteration:   8% 111/1376 [03:46<36:15,  1.72s/it]\u001b[A\n",
            "Iteration:   8% 112/1376 [03:48<35:12,  1.67s/it]\u001b[A\n",
            "Iteration:   8% 113/1376 [03:49<34:56,  1.66s/it]\u001b[A\n",
            "Iteration:   8% 114/1376 [03:51<34:42,  1.65s/it]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PUvJlHXts-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "c84ca02e-759a-419d-d48e-185b1bbae159"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.0.2-cp36-none-any.whl size=858594 sha256=218bcb5448f178fa5a20440e0d5c21e164427cd1c4981d81a93b39e1943a5e78\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o169lrfw/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed tokenizers-0.8.1rc2 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWXwEYKWvWi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}